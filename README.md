# Claude Plan Orchestrator

An autonomous development orchestrator that manages Claude Code agents working on your project. It parses markdown plans, dispatches work to agents in isolated git worktrees, and handles the full PR lifecycle through to merge.

## Features

- **Task Management**: Parse tasks from markdown plans with YAML frontmatter
- **Dependency Resolution**: Automatic implicit (sequential) and explicit dependency tracking
- **Agent Orchestration**: Spawn and manage multiple Claude Code agents in parallel
- **Git Worktree Isolation**: Each agent works in its own worktree for conflict-free development
- **PR Lifecycle**: Automatic PR creation, semantic review routing, and auto-merge for routine changes
- **Real-time Monitoring**: TUI dashboard and Web UI with live status updates
- **Notifications**: Desktop notifications and Slack webhook integration
- **Scheduled Batches**: Cron-based batch execution for overnight runs

## Quick Start

```bash
# Install (no compilation required)
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install.sh | bash

# Set up a new project
claude-orch onboard
```

## Installation

### Option 1: Quick Install (Recommended)

Download pre-built binaries with a single command. This installs both `claude-orch` (the orchestrator) and `build-mcp` (MCP server for distributed builds):

```bash
# Install latest version
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install.sh | bash

# Install specific version
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install.sh | bash -s -- v1.0.0

# Custom install directory
INSTALL_DIR=/usr/local/bin curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install.sh | bash
```

### Option 2: Homebrew (macOS/Linux)

```bash
brew tap hochfrequenz/tap
brew install claude-orch
```

### Option 3: Build from Source

Prerequisites: Go 1.21+

```bash
# Clone the repository
git clone https://github.com/hochfrequenz/claude-plan-orchestrator.git
cd claude-plan-orchestrator

# Build both binaries
go build -o claude-orch ./cmd/claude-orch
go build -o build-mcp ./cmd/build-mcp

# Install to PATH (keep both in same directory)
mv claude-orch build-mcp ~/.local/bin/
```

### Prerequisites

- Git
- Claude Code CLI (`claude`) - required for agent execution

### Check Dependencies

```bash
# Check what's installed/missing
./scripts/check-deps.sh

# Install missing dependencies (user-local)
./scripts/check-deps.sh --local

# Install missing dependencies (system-wide)
./scripts/check-deps.sh --install
```

## Project Onboarding

Set up claude-orch for a new project with the interactive wizard:

```bash
claude-orch onboard
```

This will:
1. Check prerequisites (git, claude CLI)
2. Create configuration file (~/.config/claude-plan-orchestrator/config.toml)
3. Set up plans directory structure (docs/plans/)
4. Create a sample plan file
5. Run initial task sync

Alternatively, run the standalone onboarding script:

```bash
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/onboard.sh | bash
```

## Configuration

Create a configuration file at `~/.config/claude-plan-orchestrator/config.toml`:

```toml
[general]
# Path to the your project repository
project_root = "~/code/energy-erp"

# Directory for agent worktrees
worktree_dir = "~/.claude-plan-orchestrator/worktrees"

# Maximum concurrent agents
max_parallel_agents = 3

# SQLite database path
database_path = "~/.claude-plan-orchestrator/orchestrator.db"

[claude]
model = "claude-opus-4-5-20251101"
max_tokens = 16000

[notifications]
desktop = true
slack_webhook = ""  # Optional: Slack webhook URL

[web]
host = "127.0.0.1"
port = 8080

[prompts]
# Optional: custom prompts directory
override_dir = "~/.config/claude-orchestrator/prompts"
```

### Customizing Agent Prompts

Agent prompts are embedded at compile time but can be overridden for customization. This allows you to modify the instructions given to Claude Code agents without rebuilding.

**Override Precedence** (first match wins):
1. Project-local: `.claude-orchestrator/prompts/` in your project root
2. User config: `~/.config/claude-orchestrator/prompts/`
3. Built-in defaults (embedded in binary)

**Available Prompts:**

| File | Purpose |
|------|---------|
| `epic/task.md` | Main prompt for epic task execution |
| `maintenance/wrapper.md` | Autonomous execution wrapper for maintenance tasks |
| `maintenance/{refactor,cleanup,optimize,docs,tests,security,lint}.md` | Individual maintenance task templates |
| `skills/autonomous-plan-execution.md` | Skill definition for autonomous execution |

**To customize a prompt:**

```bash
# Create override directory
mkdir -p ~/.config/claude-orchestrator/prompts/epic

# Copy the default template (view at internal/prompts/epic/task.md in source)
# Or create your own based on the template structure

# Edit the prompt
cat > ~/.config/claude-orchestrator/prompts/epic/task.md << 'EOF'
You are implementing: {{.Title}}

Epic file: {{.EpicFilePath}}

{{.EpicContent}}
{{if .ModuleContext}}
Module context:
{{.ModuleContext}}
{{end}}
Dependencies completed: {{.CompletedDeps}}

# Your custom instructions here...
EOF
```

**Template Variables:**

Epic prompts (`epic/task.md`):
- `{{.Title}}` - Task title
- `{{.EpicFilePath}}` - Path to epic file
- `{{.EpicContent}}` - Full epic markdown content
- `{{.ModuleContext}}` - Module overview (may be empty)
- `{{.CompletedDeps}}` - Comma-separated list of completed dependencies

Maintenance prompts (`maintenance/*.md`):
- `{{.Scope}}` - Scope description (e.g., "the 'api' module")
- `{{.Module}}` - Module name

Maintenance templates use YAML frontmatter for metadata:

```yaml
---
id: refactor
name: Refactor Code
description: Improve code structure without changing behavior
scopes: [module, package, all]
---
Your prompt content here with {{.Scope}} placeholder...
```

### Scheduled Batches (Optional)

Create `~/.config/claude-plan-orchestrator/schedule.toml` for automated batch runs:

```toml
[[batch]]
name = "overnight"
cron = "0 22 * * *"  # 10 PM daily
max_tasks = 10
max_duration = "8h"
notify_on_complete = true

[[batch]]
name = "weekday-noon"
cron = "0 12 * * 1-5"  # Noon on weekdays
max_tasks = 5
max_duration = "4h"
```

## Usage

### Syncing Tasks

Parse tasks from markdown files in `docs/plans/`:

```bash
claude-orch sync
```

Tasks are identified as `{module}/E{number}` (e.g., `technical/E05`).

### Viewing Status

```bash
# Summary view
claude-orch status

# List all tasks
claude-orch list

# Filter by module
claude-orch list --module technical

# Filter by status
claude-orch list --status in_progress
```

### Starting Tasks

```bash
# Start next 3 ready tasks (default)
claude-orch start

# Start specific number of tasks
claude-orch start --count 5

# Start specific tasks
claude-orch start technical/E05 billing/E02

# Start tasks from specific module
claude-orch start --module technical
```

### Viewing Logs

```bash
claude-orch logs technical/E05
```

### TUI Dashboard

Launch the terminal UI for real-time monitoring:

```bash
claude-orch tui
```

### Web UI

Start the web server:

```bash
# Default port 8080
claude-orch serve

# Custom port
claude-orch serve --port 3000
```

Then open http://localhost:8080 in your browser.

### PR Management

```bash
# List PRs needing review
claude-orch pr review

# Manually merge a flagged PR
claude-orch pr merge technical/E05
```

## Task Format

Tasks are defined in markdown files under `docs/plans/`. Example:

```markdown
---
module: technical
epic: 5
title: Input Validators
priority: high
depends_on:
  - technical/E04
  - billing/E00
---

# Epic 05: Input Validators

Implement input validation for all user-facing forms.

## Acceptance Criteria

- [ ] Email validation
- [ ] Phone number validation
- [ ] Required field validation
```

### Dependencies

- **Implicit**: Within a module, E{N} automatically depends on E{N-1}
- **Explicit**: Defined in `depends_on` frontmatter field
- **Cross-module**: Can reference tasks from other modules

## Architecture

```
internal/
â”œâ”€â”€ parser/      # Markdown parsing, YAML extraction
â”œâ”€â”€ taskstore/   # SQLite task state management
â”œâ”€â”€ scheduler/   # Dependency resolution, priority queue
â”œâ”€â”€ executor/    # Worktree management, agent spawning
â”œâ”€â”€ prbot/       # PR creation, semantic analysis, auto-merge
â”œâ”€â”€ observer/    # Log collection, metrics, stuck detection
â”œâ”€â”€ sync/        # Status writeback to markdown
â”œâ”€â”€ batch/       # Scheduled batch execution
â”œâ”€â”€ notify/      # Desktop and Slack notifications
â”œâ”€â”€ buildpool/   # Distributed build coordinator
â””â”€â”€ buildworker/ # Remote build agent

web/
â”œâ”€â”€ api/         # HTTP API with SSE
â””â”€â”€ ui/          # Svelte web dashboard
```

## Distributed Build Pool

The build pool offloads expensive operations (cargo build, cargo test, cargo clippy) from Claude Code agents to dedicated build workers. This speeds up agent execution and allows builds to run on powerful remote machines.

### Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Coordinator Host                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  claude-orch    â”‚  â”‚ Git Daemon   â”‚  â”‚  WebSocket Server      â”‚ â”‚
â”‚  â”‚  build-pool     â”‚  â”‚ :9418        â”‚  â”‚  :8081                 â”‚ â”‚
â”‚  â”‚  start          â”‚  â”‚              â”‚  â”‚                        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚           â”‚                  â”‚                      â”‚              â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                              â”‚                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                  â”‚                  â”‚
            â–¼                  â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Build Agent 1   â”‚ â”‚   Build Agent 2   â”‚ â”‚   Build Agent N   â”‚
â”‚   (build-agent)   â”‚ â”‚   (build-agent)   â”‚ â”‚   (build-agent)   â”‚
â”‚                   â”‚ â”‚                   â”‚ â”‚                   â”‚
â”‚ â€¢ Clones via git  â”‚ â”‚ â€¢ Clones via git  â”‚ â”‚ â€¢ Clones via git  â”‚
â”‚ â€¢ Runs builds     â”‚ â”‚ â€¢ Runs builds     â”‚ â”‚ â€¢ Runs builds     â”‚
â”‚ â€¢ Streams output  â”‚ â”‚ â€¢ Streams output  â”‚ â”‚ â€¢ Streams output  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### How It Works

1. **Coordinator** (`claude-orch build-pool start`) runs on the main host:
   - Exposes a WebSocket server for agent connections
   - Runs a git daemon to serve the repository to workers
   - Dispatches build jobs to available workers
   - Falls back to local execution if no workers are connected

2. **Build Agents** (`build-agent`) run on worker machines:
   - Connect to coordinator via WebSocket
   - Clone the repository via git protocol
   - Execute build commands in isolated worktrees
   - Stream output back to coordinator
   - Automatically reconnect with exponential backoff (1s â†’ 60s)

3. **Claude Code agents** use MCP tools (`build`, `test`, `clippy`) that:
   - Submit jobs to the coordinator
   - Wait for results from remote workers
   - Receive full build output

### Starting the Coordinator

Enable the build pool in your config (`~/.config/claude-plan-orchestrator/config.toml`):

```toml
[build_pool]
enabled = true
websocket_port = 8081
git_daemon_port = 9418
git_daemon_listen_addr = ""  # Empty = all interfaces, "127.0.0.1" = local only

[build_pool.local_fallback]
enabled = true              # Run builds locally if no workers connected
max_jobs = 2
worktree_dir = "/tmp/build-pool/local"

[build_pool.timeouts]
job_default_secs = 300      # 5 minute default timeout
heartbeat_interval_secs = 30
heartbeat_timeout_secs = 90 # Allow missing 2 heartbeats (handles high CPU load)
```

Start the coordinator (or just launch the TUI - it auto-starts the coordinator):

```bash
claude-orch build-pool start
```

Output:
```
Build pool coordinator starting...
  WebSocket: :8081
  Git daemon: :9418
```

### Deploying Build Agents

#### Prerequisites

Build agents require **Nix** to be installed. Jobs run inside `nix develop` to ensure reproducible builds with all dependencies available.

**Install Nix:**

```bash
# Linux/macOS (recommended: Determinate Nix Installer)
curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install

# Or official installer
sh <(curl -L https://nixos.org/nix/install) --daemon
```

After installation, ensure `nix` is in your PATH:

```bash
# Verify installation
nix --version

# If not found, source the profile
. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh
```

**Additional requirements:**
- Git (for cloning repositories)
- Network access to the coordinator host

#### Option 1: Quick Install (Recommended)

Download a pre-built binary:

```bash
# Install latest version
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install-build-agent.sh | bash

# Install specific version
curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install-build-agent.sh | bash -s -- v1.0.0

# Custom install directory
INSTALL_DIR=/opt/bin curl -fsSL https://raw.githubusercontent.com/hochfrequenz/claude-plan-orchestrator/main/scripts/install-build-agent.sh | bash
```

#### Option 2: Build from Source

Build and run the agent binary:

```bash
# Build
go build -o build-agent ./cmd/build-agent

# Run with flags
./build-agent --server ws://coordinator:8081/ws --id worker-1 --jobs 4

# Or just run - config is auto-discovered from default locations
./build-agent
```

The agent automatically looks for config files in these locations (in order):
1. `/etc/build-agent/config.toml`
2. `/etc/build-agent.toml`

You can also specify a custom path with `--config /path/to/config.toml`.

Agent config file (`/etc/build-agent/config.toml`):

```toml
# Single orchestrator (legacy format, still supported)
[server]
url = "ws://coordinator-host:8081/ws"

[worker]
id = "worker-1"      # Defaults to hostname
max_jobs = 4         # Concurrent build jobs

[storage]
git_cache_dir = "/var/cache/build-agent/repos"
worktree_dir = "/tmp/build-agent/jobs"

[nix]
# Prewarm nix store on startup with common packages
# This speeds up first job by pre-downloading toolchains
prewarm_packages = [
  "nixpkgs#rustc",
  "nixpkgs#cargo",
  "nixpkgs#clippy",
  "nixpkgs#rustfmt"
]
```

**Multi-Orchestrator Configuration:**

A single build agent can connect to multiple orchestrators simultaneously, accepting work from any of them while sharing a common job pool:

```toml
# Multiple orchestrators - agent connects to all simultaneously
[[servers]]
url = "ws://orchestrator1:8081/ws"
name = "project-a"  # Optional, for logging

[[servers]]
url = "ws://orchestrator2:8081/ws"
name = "project-b"

[[servers]]
url = "ws://orchestrator3:8081/ws"
# name is optional, defaults to URL in logs

[worker]
id = "shared-worker"
max_jobs = 8         # Total capacity shared across all orchestrators

[storage]
git_cache_dir = "/var/cache/build-agent/repos"
worktree_dir = "/tmp/build-agent/jobs"
```

With multi-orchestrator mode:
- The agent maintains separate WebSocket connections to each orchestrator
- Job capacity is shared: if `max_jobs = 8` and orchestrator1 assigns 3 jobs, orchestrators 2 and 3 see 5 available slots
- Each orchestrator receives `ReadyMessage` updates when slot availability changes
- Connections are independent: if one orchestrator goes down, the agent continues serving others
- Automatic reconnection applies per-connection with exponential backoff

#### Option 3: NixOS Module

For NixOS systems, use the provided module:

```nix
# configuration.nix
{ config, pkgs, ... }:

{
  imports = [ ./path/to/nix/build-agent.nix ];

  services.build-agent = {
    enable = true;
    serverUrl = "ws://coordinator:8081/ws";
    package = pkgs.build-agent;  # Your build-agent package
    maxJobs = 4;
    gitCacheDir = "/var/cache/build-agent/repos";
    worktreeDir = "/tmp/build-agent/jobs";
  };
}
```

The NixOS module provides:
- Systemd service with automatic restart
- Security hardening (DynamicUser, NoNewPrivileges, ProtectSystem)
- Automatic directory creation

#### Option 4: Systemd Service (Manual)

Create `/etc/systemd/system/build-agent.service`:

```ini
[Unit]
Description=Build Agent Worker
After=network-online.target
Wants=network-online.target

[Service]
Type=simple
ExecStart=/usr/local/bin/build-agent --config /etc/build-agent/config.toml
Restart=always
RestartSec=10
User=build-agent

[Install]
WantedBy=multi-user.target
```

Enable and start:

```bash
sudo systemctl enable build-agent
sudo systemctl start build-agent
```

### Agent Features

- **Automatic Reconnection**: If the coordinator restarts, agents reconnect with exponential backoff (1s, 2s, 4s, ... up to 60s max)
- **Job Cancellation**: Long-running builds can be cancelled via the coordinator
- **Output Streaming**: Build output streams back in real-time
- **Concurrent Jobs**: Each agent can run multiple jobs in parallel
- **Git Caching**: Repository clones are cached to speed up subsequent jobs
- **Nix Store Prewarm**: Optionally pre-download common toolchains at startup to speed up first job

### Monitoring Workers

Check connected workers via the MCP `worker_status` tool or TUI dashboard:

```json
{
  "workers": [
    {
      "id": "worker-1",
      "max_jobs": 4,
      "active_jobs": 2,
      "connected_since": "2024-01-15T10:30:00Z"
    }
  ],
  "queued_jobs": 0,
  "local_fallback_active": false
}
```

### Security Considerations

- **Git Daemon**: By default listens on all interfaces. Set `git_daemon_listen_addr = "127.0.0.1"` for local-only access, or use a VPN/firewall for remote workers.
- **WebSocket**: No authentication yet. Run behind a reverse proxy with TLS for production, or restrict to trusted networks.
- **Worker Isolation**: Each build runs in an isolated git worktree that's cleaned up after completion.

## Semantic Review Routing

PRs are automatically merged unless changes touch sensitive areas:

| Category | Triggers |
|----------|----------|
| **Security** | Auth, encryption, credentials, permissions |
| **Architecture** | Core modules, new dependencies, public API |
| **Migrations** | Database schema changes |

Flagged PRs require manual review and merge.

## Status Legend

| Emoji | Status |
|-------|--------|
| ðŸ”´ | Not started |
| ðŸŸ¡ | In progress |
| ðŸŸ¢ | Complete |

## Development

```bash
# Run tests
go test ./...

# Run specific test
go test -run TestName ./path/to/package

# Lint
golangci-lint run

# Build Web UI (for development)
cd web/ui
npm install
npm run dev
```

## License

MIT
